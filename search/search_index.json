{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>adaXT is a Python module for tree-based machine-learning algorithms that is fast, adaptable and extendable. It aims to provide researchers a more flexible workflow when developing tree-based models.</p> <p>It is distributed under the 3-Clause BSD license.</p> <p>We encourage users and developers to report problems, request features, ask for help, or leave general comments.</p> <p>Website: https://NiklasPfister.github.io/adaXT</p>"},{"location":"#overview","title":"Overview","text":"<p>adaXT implements several tree types that can be used out-of-the-box, both as decision trees and random forests. Currently, the following tree types are implemented:</p> <ul> <li>Classification: For prediction tasks in which the response is categorical.</li> <li>Regression: For prediction tasks in which the response is continuous.</li> <li>Quantile: For uncertainty quantification tasks in which the response is   continuous and the goal is to estimate one or more quantiles of the   conditional distribution of the response given the predictors.</li> <li>Gradient: For tasks in which one aims to estimate (directional)   derivatives of the response given the predictors. A related tree type is used   in the   Xtrapolation   method.</li> </ul> <p>Beyond these pre-defined tree types, adaXT offers a simple interface to extend or modify most components of the tree models. For example, it is easy to create a custom criteria function that is used to create splits.</p>"},{"location":"#getting-started","title":"Getting started","text":"<p>adaXT is available on pypi and can be installed via pip</p> <pre><code>pip install adaXT\n</code></pre> <p>Working with any of the default tree types uses the same class-style interface as other popular machine learning packages. The following code illustrates this for <code>Regression</code> and <code>Quantile</code> random forests:</p> <pre><code>from adaXT.random_forest import RandomForest\nimport numpy as np\n\n# Create toy regression data\nn = 100\nX = np.random.normal(0, 1, (n, 2))\nY = X[:, 0] + np.random.normal(0, 1, n)\nXtest = np.c_[np.linspace(-1, 1, n), np.random.uniform(0, 1, n)]\n\n# Task 1: Fit regression forest\nrf = RandomForest(\"Regression\")\nrf.fit(X, Y)\n\n# Predict on test data\nYpred = rf.predict(Xtest)\n\n# Predict forest weight on X or Xtest\n# -- can be used a similarity measure on the predictor space\nweight_train = rf.predict_weights()\nweight_test = rf.predict_weights(Xtest)\n\n# Task 2: Fit a quantile regression\nqf = RandomForest(\"Quantile\")\nqf.fit(X, Y)\n\n# Predict 10% and 90% conditional quantile on test data\nYbdd = qf.predict(Xtest, quantile=[0.1, 0.9])\n</code></pre> <p>The main advantage of adaXT over existing tree-based ML packages is its modularity and extendability, which is discussed in detail in the documentation.</p>"},{"location":"#project-goals","title":"Project goals","text":"<p>This project aims to provide a flexible and unified code-base for various tree-based algorithms that strikes a balance between speed and ease with which the code can be adapted and extended. It should provide researchers a simple toolkit for prototyping new tree-based algorithms.</p> <p>adaXT provides an intuitive user experience that is similar to the scikit-learn implementation of decision trees both in terms of model-based syntax and hyperparameters. Under the hood, however, adaXT strikes a different balance between speed and ease of adapting and extending the code.</p>"},{"location":"#adaptable-and-extendable","title":"Adaptable and extendable","text":"<p>At the heart of any tree-based algorithm is a decision tree that can be fitted on data and then used to perform some version of prediction. adaXT has therefore been designed with a modular decision tree implementation that takes four input components:</p> <ul> <li> <p>Criteria class: Used during fitting to determine splits.</p> </li> <li> <p>LeafBuilder class: Used during fitting to specify what is saved on the leaf   nodes.</p> </li> <li> <p>Splitter class: Used during fitting to perform the splits.</p> </li> <li> <p>Predict class: Used after fitting to make predictions.</p> </li> </ul> <p>By specifying these four components a range of different tree algorithms can be created, e.g., regression trees, classification trees, quantile regression trees and gradient trees. Additionally to this modular structure, all other operations are kept as vanilla as possible allowing users to easily change parts of the code (e.g., the splitting procedure).</p>"},{"location":"#speed","title":"Speed","text":"<p>As tree-based algorithms involve expensive loops over the training dataset, it is important that these computations are implemented in a compiled language. adaXT implements all computationally expensive operations in Cython. This results in speeds similar (although a few factors slower) than the corresponding scikit-learn implementations. However, due to its modular structure and the avoidance of technical speed-ups, adaXT does not intend to provide state-of-the-art speed and users mainly concerned with speed should consider more targeted implementations.</p>"},{"location":"api_docs/Criteria/","title":"Criteria Class","text":"<p>This class is used to evaluate the quality of a split while fitting a decision tree.</p> <p>There are several default Criteria classes implemented in adaXT that can be directly loaded. It is also possible to create a custom criteria class, which is explained here.</p> <p>Criteria classes can be loaded as follows: <pre><code>from adaXT.criteria import CRITERIA_NAME\n</code></pre></p> <p>A list of all available Criteria classes is given below.</p>"},{"location":"api_docs/Criteria/#adaXT.criteria.criteria.ClassificationCriteria","title":"ClassificationCriteria","text":"<p>Parent class for Criteria used in the Classification Tree Type. Can not be used as a standalone Criteria.</p>"},{"location":"api_docs/Criteria/#adaXT.criteria.criteria.Criteria","title":"Criteria","text":"<p>The base Criteria class from which all other criteria need to inherit.</p>"},{"location":"api_docs/Criteria/#adaXT.criteria.criteria.Entropy","title":"Entropy","text":"<p>Entropy based criteria, which can be used for classification. Formally, given class labels \\(\\mathcal{L}\\), the entropy in a node consisting of samples \\(I\\), is given by $$ \\text{Entropy} = - \\sum_{k\\in\\mathcal{L}} P[k] \\log_2 (P[k]), $$ where \\(P[k]\\) denotes the fraction of samples in \\(I\\) with class label \\(k\\).</p>"},{"location":"api_docs/Criteria/#adaXT.criteria.criteria.GiniIndex","title":"GiniIndex","text":"<p>Gini index based criteria, which can be used for classification. Formally, given class labels \\(\\mathcal{L}\\), the Gini index in a node consisting of samples \\(I\\), is given by $$ \\text{GiniIndex} = 1 - \\sum_{k\\in \\mathcal{L}} P[k]^2, $$ where \\(P[k]\\) denotes the fraction of samples in \\(I\\) with class label \\(k\\).</p>"},{"location":"api_docs/Criteria/#adaXT.criteria.criteria.MultiSquaredError","title":"MultiSquaredError","text":"<p>Multi dimensional squared error criteria. With Y-values in one-dimension, it is equivalent to the SquaredError criteria. However, this criteria is able to function with Y-values in multiple dimensions. Formally, the MultiSquaredError in a node consisting of samples \\(I\\) and Y-values of \\(D\\) dimensions, is given by: $$ \\text{MultiSquaredError} = \\tfrac{1}{|I|} \\sum^D_{j = 1} \\sum_{i \\in I} \\Left(Y[i, j] - \\tfrac{1}{|I|\\sum_{i \\in I} Y[I] \\Right)^2 $$</p> <p>For a faster, but equivalent calculation, it is computed as: $$ \\text{MultiSquaredError} = \\tfrac{1}{|I|} \\sum^D_{j = 1} \\left(\\sum_{i\\in I} Y[i]^2 - \\Big(\\tfrac{1}{|I|}\\sum_{i\\in I} Y[i]\\Big)^2 \\right) $$</p>"},{"location":"api_docs/Criteria/#adaXT.criteria.criteria.PairwiseEuclideanDistance","title":"PairwiseEuclideanDistance","text":"<p>Pairwise Euclidean Distance criteria. Generally performs in a similair fashion to the MultiSquaredError. However, instead of the squared error compared with the mean, it instead minimizes the individual distance between points in a node. Formally, the PairwiseEuclideanDistance in a node consisting of samples \\(I\\) and Y-values of \\(D\\) dimensions is given by: $$ \\text{PairwiseEuclideanDistance} = \\tfrac{1}{|I|} \\sum_{i = 1}^{|I| -1} \\sum_{j = i}^{|I|} \\sqrt{\\sum_{k = 1}^{D} (Y[I[i], k] - Y[I[j], k])^2} $$</p>"},{"location":"api_docs/Criteria/#adaXT.criteria.criteria.PartialLinear","title":"PartialLinear","text":"<p>Criteria based on fitting a linear function in the first predictor variable in each leaf. Formally, in a node consisting of samples \\(I\\), it is given by $$ \\text{PartialLinear} = \\tfrac{1}{|I|}\\sum_{i \\in I} (Y[i] - \\widehat{\\theta}_0 - \\widehat{\\theta}_1 X[i, 0])^2, $$ where \\(Y[i]\\) and \\(X[i, 0]\\) denote the response value and the value of the first feature at sample \\(i\\), respectively, and \\((\\widehat{\\theta}_0, \\widehat{\\theta}_1)\\) are ordinary least squares regression estimates when regressing \\(Y[i]\\) on \\(X[i, 0]\\) using the samples in \\(I\\).</p>"},{"location":"api_docs/Criteria/#adaXT.criteria.criteria.PartialQuadratic","title":"PartialQuadratic","text":"<p>Criteria based on fitting a quadratic function in the first predictor variable in each leaf. Formally, in a node consisting of samples \\(I\\), it is given by $$ \\text{PartialQuadratic} = \\tfrac{1}{|I|}\\sum_{i \\in I} (Y[i] - \\widehat{\\theta}_0 - \\widehat{\\theta}_1 X[i, 0] - \\widehat{\\theta}_2 X[i, 0]^2)^2, $$ where \\(Y[i]\\) and \\(X[i, 0]\\) denote the response value and the value of the first feature at sample \\(i\\), respectively, and \\((\\widehat{\\theta}_0, \\widehat{\\theta}_1)\\) are ordinary least squares regression estimates when regressing \\(Y[i]\\) on \\(X[i, 0]\\) using the samples in \\(I\\).</p>"},{"location":"api_docs/Criteria/#adaXT.criteria.criteria.RegressionCriteria","title":"RegressionCriteria","text":"<p>Parent class for criteria used in Regression Tree Type. Can not be used as a standalone Criteria.</p>"},{"location":"api_docs/Criteria/#adaXT.criteria.criteria.SquaredError","title":"SquaredError","text":"<p>Squared error based criteria, which can be used for regression and leads to standard CART splits. Formally, the squared error in a node consisting of samples \\(I\\), is given by $$ \\text{SquaredError} = \\tfrac{1}{|I|}\\sum_{i\\in I} \\Big(Y[i] - \\tfrac{1}{|I|}\\sum_{i\\in I} Y[i]\\Big)^2, $$ where \\(Y[i]\\) denotes the response value at sample \\(i\\).</p> <p>For a faster, but equivalent calculation, it is computed by $$ \\text{Squared_error} = \\tfrac{1}{|I|}\\sum_{i\\in I} Y[i]^2 - \\Big(\\tfrac{1}{|I|}\\sum_{i\\in I} Y[i]\\Big)^2 $$</p>"},{"location":"api_docs/DecisionTree/","title":"DecisionTree Class","text":"<p>This is the class used to construct a decision tree. It uses the following four individual components to construct specific types of decision trees that can then be applied to data.</p> <ul> <li>Criteria</li> <li>LeafBuilder</li> <li>Predictor</li> </ul> <p>Instead of the user specifying all three components individually, it is also possible to only specify the <code>tree_type</code>, which then internally selects the corresponding default components for several established tree-algorithms, see user guide.</p> <p>For more advanced modifications, it might be necessary to change how the splitting is performed. This can be done by passing a custom Splitter class.</p> <p>The DecisionTree class and can be imported as follows:</p> <pre><code>from adaXT.decision_tree import DecisionTree\n</code></pre> <p>Attributes:</p> Name Type Description <code>max_depth</code> <code>int</code> <p>The maximum depth of the tree.</p> <code>tree_type</code> <code>str</code> <p>The type of tree, either a string specifying a supported type (currently \"Regression\", \"Classification\", \"Quantile\" or \"Gradient\") or None.</p> <code>leaf_nodes</code> <code>list[LeafNode]</code> <p>A list of all leaf nodes in the tree.</p> <code>root</code> <code>Node</code> <p>The root node of the tree.</p> <code>n_nodes</code> <code>int</code> <p>The number of nodes in the tree.</p> <code>n_features</code> <code>int</code> <p>The number of features in the training data.</p> <code>n_rows</code> <code>int</code> <p>The number of rows (i.e., samples) in the training data.</p> <p>Parameters:</p> Name Type Description Default <code>tree_type</code> <code>str | None</code> <p>The type of tree, either a string specifying a supported type (currently \"Regression\", \"Classification\", \"Quantile\" or \"Gradient\") or None.</p> <code>None</code> <code>max_depth</code> <code>int</code> <p>The maximum depth of the tree.</p> <code>maxsize</code> <code>impurity_tol</code> <code>float</code> <p>The tolerance of impurity in a leaf node.</p> <code>0</code> <code>max_features</code> <code>int | float | Literal['sqrt', 'log2'] | None</code> <p>The number of features to consider when looking for a split.</p> <code>None</code> <code>min_samples_split</code> <code>int</code> <p>The minimum number of samples in a split.</p> <code>1</code> <code>min_samples_leaf</code> <code>int</code> <p>The minimum number of samples in a leaf node.</p> <code>1</code> <code>min_improvement</code> <code>float</code> <p>The minimum improvement gained from performing a split.</p> <code>0</code> <code>criteria</code> <code>Type[Criteria] | None</code> <p>The Criteria class to use, if None it defaults to the tree_type default.</p> <code>None</code> <code>leaf_builder</code> <code>Type[LeafBuilder] | None</code> <p>The LeafBuilder class to use, if None it defaults to the tree_type default.</p> <code>None</code> <code>predictor</code> <code>Type[Predictor] | None</code> <p>The Predictor class to use, if None it defaults to the tree_type default.</p> <code>None</code> <code>splitter</code> <code>Type[Splitter] | None</code> <p>The Splitter class to use, if None it defaults to the default Splitter class.</p> <code>None</code> <code>skip_check_input</code> <code>bool</code> <p>Skips any error checking on the features and response in the fitting function of a tree, should only be used if you know what you are doing, by default false.</p> <code>False</code>"},{"location":"api_docs/DecisionTree/#adaXT.decision_tree.DecisionTree.fit","title":"fit","text":"<pre><code>fit(X, Y, sample_indices=None, sample_weight=None)\n</code></pre> <p>Fit the decision tree with training data (X, Y).</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like object of dimension 2</code> <p>The feature values used for training. Internally it will be converted to np.ndarray with dtype=np.float64.</p> required <code>Y</code> <code>array-like object of 1 or 2 dimensions</code> <p>The response values used for training. Internally it will be converted to np.ndarray with dtype=np.float64.</p> required <code>sample_indices</code> <code>array-like object of dimension 1 | None</code> <p>A vector specifying samples of the training data that should be used during training. If None all samples are used.</p> <code>None</code> <code>sample_weight</code> <code>array-like object of dimension 1 | None</code> <p>Sample weights. May not be implemented for every criteria.</p> <code>None</code>"},{"location":"api_docs/DecisionTree/#adaXT.decision_tree.DecisionTree.predict","title":"predict","text":"<pre><code>predict(X, **kwargs)\n</code></pre> <p>Predict response values at X using fitted decision tree. The behavior of this function is determined by the Prediction class used in the decision tree. For currently existing tree types the corresponding behavior is as follows:</p> Classification: <p>Returns the class with the highest proportion within the final leaf node.</p> <p>Given predict_proba=True, it instead calculates the probability distribution.</p> Regression: <p>Returns the mean value of the response within the final leaf node.</p> Quantile: <p>Returns the conditional quantile of the response, where the quantile is specified by passing a list of quantiles via the <code>quantile</code> parameter.</p> Gradient: <p>Returns a matrix with columns corresponding to different orders of derivatives that can be provided via the 'orders' parameter. Default behavior is to compute orders 0, 1 and 2.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like object of dimension 2</code> <p>New samples at which to predict the response. Internally it will be converted to np.ndarray with dtype=np.float64.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>(N, K) numpy array with the prediction, where K depends on the Prediction class and is generally 1</p>"},{"location":"api_docs/DecisionTree/#adaXT.decision_tree.DecisionTree.predict_leaf","title":"predict_leaf","text":"<pre><code>predict_leaf(X)\n</code></pre> <p>Computes a hash table indexing in which LeafNodes the rows of the provided X fall into.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like object of dimension 2</code> <p>2-dimensional array for which the rows are the samples at which to predict.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A hash table with keys corresponding to LeafNode ids and values corresponding to lists of indices of the rows that land in a given LeafNode.</p>"},{"location":"api_docs/DecisionTree/#adaXT.decision_tree.DecisionTree.predict_weights","title":"predict_weights","text":"<pre><code>predict_weights(X=None, scale=True)\n</code></pre> <p>Predicts a weight matrix W, where W[i,j] indicates if X[i, :] and Xtrain[j, :] are in the same leaf node, where Xtrain denotes the training data. If scale is True, then the value is divided by the number of other training samples in the same leaf node.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ArrayLike | None</code> <p>New samples to predict a weight (corresponding to columns in the output). If None then the training data is used as X.</p> <code>None</code> <code>scale</code> <code>bool</code> <p>Whether to do row-wise scaling.</p> <code>True</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A numpy array of shape MxN, where N denotes the number of rows of the original training data and M the number of rows of X.</p>"},{"location":"api_docs/DecisionTree/#adaXT.decision_tree.DecisionTree.refit_leaf_nodes","title":"refit_leaf_nodes","text":"<pre><code>refit_leaf_nodes(X, Y, sample_weight=None, sample_indices=None)\n</code></pre> <p>Refits the leaf nodes in a previously fitted decision tree.</p> <p>More precisely, the method removes all leafnodes created on the initial fit and replaces them by predicting all samples in X that appear in sample_indices and placing them into new leaf nodes.</p> <p>This method can be used to update the leaf nodes in a decision tree based on a new data while keeping the original splitting rules. If X does not contain the original training data the tree structure might change as leaf nodes without samples are collapsed. The method is also used to create honest splitting in RandomForests.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like object of dimension 2</code> <p>The feature values used for training. Internally it will be converted to np.ndarray with dtype=np.float64.</p> required <code>Y</code> <code>array-like object of dimension 1 or 2</code> <p>The response values used for training. Internally it will be converted to np.ndarray with dtype=np.float64.</p> required <code>sample_weight</code> <code>array-like object of dimension 1 | None</code> <p>Sample weights. May not be implemented for all criteria.</p> <code>None</code> <code>sample_indices</code> <code>ArrayLike | None</code> <p>Indices of X which to create new leaf nodes with.</p> <code>None</code>"},{"location":"api_docs/DecisionTree/#adaXT.decision_tree.DecisionTree.similarity","title":"similarity","text":"<pre><code>similarity(X0, X1)\n</code></pre> <p>Computes a similarity matrix W of size NxM, where each element W[i, j] is 1 if and only if X0[i, :] and X1[j, :] end up in the same leaf node.</p> <p>Parameters:</p> Name Type Description Default <code>X0</code> <code>ArrayLike</code> <p>Array corresponding to rows of W in the output.</p> required <code>X1</code> <code>ArrayLike</code> <p>Array corresponding to columns of W in the output.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>A NxM shaped np.ndarray.</p>"},{"location":"api_docs/LeafBuilder/","title":"LeafBuilder Class","text":""},{"location":"api_docs/LeafBuilder/#adaXT.leaf_builder.leaf_builder.LeafBuilder","title":"LeafBuilder","text":"<pre><code>LeafBuilder(X, Y, all_idx, **kwargs)\n</code></pre> <p>The base LeafBuilder class from which all other leaf builders must inherit.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>The feature values used for training.</p> required <code>Y</code> <code>ndarray</code> <p>The response values used for training.</p> required <code>all_idx</code> <code>ndarray</code> <p>A vector specifying samples of the training data that should be considered by the LeafBuilder.</p> required"},{"location":"api_docs/LeafBuilder/#adaXT.leaf_builder.leaf_builder.LeafBuilder.build_leaf","title":"build_leaf","text":"<pre><code>build_leaf(leaf_id, indices, depth, impurity, weighted_samples, parent)\n</code></pre> <p>Builds a leaf node.</p> <p>Parameters:</p> Name Type Description Default <code>leaf_id</code> <code>int</code> <p>unique identifier of leaf node</p> required <code>indices</code> <code>ndarray</code> <p>indices in leaf node</p> required <code>depth</code> <code>int</code> <p>depth of leaf node</p> required <code>impurity</code> <code>float</code> <p>impurity of leaf node</p> required <code>weighted_samples</code> <code>float</code> <p>summed weight of all samples in the LeafNode</p> required <code>parent</code> <code>DecisionNode</code> <p>parent node</p> required <p>Returns:</p> Type Description <code>Node</code> <p>built leaf node</p>"},{"location":"api_docs/Nodes/","title":"The Node class","text":"<p>These are the collection of different implemented Nodes used by the DecisionTree.</p>"},{"location":"api_docs/Nodes/#adaXT.decision_tree.nodes.DecisionNode","title":"DecisionNode","text":"<pre><code>DecisionNode(indices, depth, impurity, threshold, split_idx, left_child=None, right_child=None, parent=None)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>indices</code> <code>ndarray</code> <p>indices in node</p> required <code>depth</code> <code>int</code> <p>depth of node</p> required <code>impurity</code> <code>float</code> <p>impurity in node</p> required <code>threshold</code> <code>float</code> <p>threshold value of a split</p> required <code>split_idx</code> <code>int</code> <p>feature index to split on</p> required <code>left_child</code> <code>DecisionNode | LeafNode | None</code> <p>left child</p> <code>None</code> <code>right_child</code> <code>DecisionNode | LeafNode | None</code> <p>right child</p> <code>None</code> <code>parent</code> <code>DecisionNode | None</code> <p>parent node</p> <code>None</code>"},{"location":"api_docs/Nodes/#adaXT.decision_tree.nodes.LeafNode","title":"LeafNode","text":"<pre><code>LeafNode(id, indices, depth, impurity, weighted_samples, value, parent)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>int</code> <p>unique identifier of leaf node</p> required <code>indices</code> <code>ndarray</code> <p>indices in leaf node</p> required <code>depth</code> <code>int</code> <p>depth of leaf node</p> required <code>impurity</code> <code>float</code> <p>impurity of leaf node</p> required <code>weighted_samples</code> <code>float</code> <p>summed weight of all samples in leaf node</p> required <code>value</code> <code>ndarray</code> <p>value of leaf node (depends on LeafBuilder)</p> required <code>parent</code> <code>DecisionNode</code> <p>parent node</p> required"},{"location":"api_docs/Nodes/#adaXT.decision_tree.nodes.LinearPolynomialLeafNode","title":"LinearPolynomialLeafNode","text":"<pre><code>LinearPolynomialLeafNode(id, indices, depth, impurity, weighted_samples, value, parent, theta0, theta1, theta2)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>int</code> <p>unique identifier of leaf node</p> required <code>indices</code> <code>ndarray</code> <p>indices in leaf node</p> required <code>depth</code> <code>int</code> <p>depth of leaf node</p> required <code>impurity</code> <code>float</code> <p>impurity of leaf node</p> required <code>weighted_samples</code> <code>float</code> <p>summed weight of all samples in leaf node</p> required <code>value</code> <code>ndarray</code> <p>value of leaf node (depends on LeafBuilder)</p> required <code>parent</code> <code>DecisionNode</code> <p>parent node</p> required <code>theta0</code> <code>float</code> <p>theta0 parameter corresponding to intercept term</p> required <code>theta1</code> <code>float</code> <p>theta1 parameter correponding to linear term</p> required <code>theta2</code> <code>float</code> <p>theta2 parameter correponding to quadratic term</p> required"},{"location":"api_docs/Nodes/#adaXT.decision_tree.nodes.Node","title":"Node","text":"<pre><code>Node(indices, depth, impurity)\n</code></pre> <p>The base Node from which all other nodes must inherit.</p> <p>Parameters:</p> Name Type Description Default <code>indices</code> <code>ndarray</code> <p>indices in node</p> required <code>depth</code> <code>int</code> <p>depth of node</p> required <code>impurity</code> <code>float</code> <p>impurity of node</p> required"},{"location":"api_docs/Parallel/","title":"ParallelModel class","text":"<p>This model is created together with the RandomForest. It is later passed to the Predictor class as input to the static method forest_predictor.</p>"},{"location":"api_docs/Parallel/#adaXT.parallel.ParallelModel","title":"ParallelModel","text":"<pre><code>ParallelModel()\n</code></pre> <p>Class used to parallelize</p> <p>Parameters:</p> Name Type Description Default <code>n_jobs</code> <code>int</code> <p>The number of processes used to fit, and predict for the forest, -1 uses all available proccesors</p> <code>Number of cpu cores</code>"},{"location":"api_docs/Parallel/#adaXT.parallel.ParallelModel.apply","title":"apply","text":"<pre><code>apply(function, n_iterations, n_jobs=1, **kwargs)\n</code></pre> <p>Applies the function n_iterations number of times and returns the result of the n_iterations where element i corresponds to the i'th return value of function.</p> <p>Parameters:</p> Name Type Description Default <code>function</code> <code>Callable</code> <p>Function to apply</p> required <code>n_iterations</code> <code>int</code> <p>Number  of applications of function</p> required <p>Returns:</p> Type Description <code>Iterable</code> <p>Function applied n_iterations number of times</p>"},{"location":"api_docs/Parallel/#adaXT.parallel.ParallelModel.async_apply","title":"async_apply","text":"<pre><code>async_apply(function, n_iterations, n_jobs=1, **kwargs)\n</code></pre> <p>Applies the function n_iterations number of times and returns the result of the n_iterations in an unknown order.</p> <p>Parameters:</p> Name Type Description Default <code>function</code> <code>Callable</code> <p>Function to apply</p> required <code>n_iterations</code> <code>int</code> <p>Number of applications of function</p> required <p>Returns:</p> Type Description <code>Iterable</code> <p>Function applied n_iterations number of times</p>"},{"location":"api_docs/Parallel/#adaXT.parallel.ParallelModel.async_map","title":"async_map","text":"<pre><code>async_map(function, map_input, n_jobs=1, **kwargs)\n</code></pre> <p>Asynchronously applies the function to the map_input passing along any kwargs given to the function.</p> <p>Parameters:</p> Name Type Description Default <code>function</code> <code>Callable</code> <p>Function to apply Asynchronously</p> required <code>map_input</code> <code>Iterable</code> <p>Iterable input which can be passed to the function</p> required <p>Returns:</p> Type Description <code>Iterable</code> <p>Returns the result of running function on all elements of map_input</p>"},{"location":"api_docs/Parallel/#adaXT.parallel.ParallelModel.async_starmap","title":"async_starmap","text":"<pre><code>async_starmap(function, map_input, n_jobs=1, **kwargs)\n</code></pre> <p>Asynchronously apply function to map_input, where map_input might be a list of tuple elements. Passes along any kwargs to function.</p> <p>Parameters:</p> Name Type Description Default <code>function</code> <code>Callable</code> <p>Function to apply to each element of map_input</p> required <code>map_input</code> <code>Iterable</code> <p>Iterable input which might be a tuple, that can be passed to function</p> required <p>Returns:</p> Type Description <code>Iterable</code> <p>Returns the result of applying function to each element of map_input</p>"},{"location":"api_docs/Parallel/#adaXT.parallel.ParallelModel.map","title":"map","text":"<pre><code>map(function, map_input, n_jobs=1, **kwargs)\n</code></pre> <p>Maps the function with map_input. Similair to async_map, but instead guarantees that the first element returned is the result of the first map_input. Passes along any kwargs to function.</p> <p>Parameters:</p> Name Type Description Default <code>function</code> <code>Callable</code> <p>function to apply</p> required <code>map_input</code> <code>Iterable</code> <p>Iterable input which can be passed to the function</p> required <p>Returns:</p> Type Description <code>Iterable</code> <p>Returns in order the results of function applied to map_input</p>"},{"location":"api_docs/Parallel/#adaXT.parallel.ParallelModel.starmap","title":"starmap","text":"<pre><code>starmap(function, map_input, n_jobs=1, **kwargs)\n</code></pre> <p>Applies function to each elemetn of map_input but guarantees that element i of return value is the result of function applied to element i of map_input. Can be a list of tuples as opposed to just map. Passes along any kwargs to function.</p> <p>Parameters:</p> Name Type Description Default <code>function</code> <code>Callable</code> <p>Function to apply to each element of map_input</p> required <code>map_input</code> <code>Iterable</code> <p>Iterable input which might be a tuple, that can be passed to function</p> required <p>Returns:</p> Type Description <code>Iterable</code> <p>Returns the result of applying function to each element of map_input</p>"},{"location":"api_docs/Predictor/","title":"Predictor Class","text":"<p>The predictor class is used for customizing how tree.predict functions. The defaults can be seen below.</p>"},{"location":"api_docs/Predictor/#adaXT.predictor.predictor.Predictor","title":"Predictor","text":"<pre><code>Predictor(X, Y, root)\n</code></pre> <p>The base Predictor class from which all other predict classes need to inhert.</p>"},{"location":"api_docs/Predictor/#adaXT.predictor.predictor.Predictor.forest_predict","title":"forest_predict  <code>staticmethod</code>","text":"<pre><code>forest_predict(X_train, Y_train, X_pred, trees, parallel, **kwargs)\n</code></pre> <p>Internal function used by the RandomForest class 'predict' method to evaluate predictions for each tree and aggregate them.</p> <p>Needs to be adjusted whenever RandomForest predictions do not simply aggregate the tree predictions by averaging.</p> <p>Parameters:</p> Name Type Description Default <code>X_train</code> <code>ndarray</code> <p>Array of feature values used during training.</p> required <code>Y_train</code> <code>ndarray</code> <p>Array of response values used during training.</p> required <code>X_pred</code> <code>ndarray</code> <p>Array of new feature values at which to predict.</p> required <code>trees</code> <code>list[DecisionTree]</code> <p>List of fitted DecisionTrees fitted within the random forest.</p> required <code>parallel</code> <code>ParallelModel</code> <p>ParallelModel used for multiprocessing.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>An array with predictions for each row of X_pred.</p>"},{"location":"api_docs/Predictor/#adaXT.predictor.predictor.Predictor.predict","title":"predict","text":"<pre><code>predict(X, **kwargs)\n</code></pre> <p>Prediction function called by the DecisionTree when it is told to predict. Can be customised for a different output of the DecisionTree.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>A 2-dimensional array for which the rows are the samples at which to predict.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>An array with predictions for each row of X.</p>"},{"location":"api_docs/Predictor/#adaXT.predictor.predictor.Predictor.predict_leaf","title":"predict_leaf","text":"<pre><code>predict_leaf(X)\n</code></pre> <p>Computes hash table indexing in which LeafNodes the rows of X fall into.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>2-dimensional array for which the rows are the samples at which to predict.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A hash table with keys corresponding to LeafNode ids and values lists of indices specifying which rows land in a given LeafNode.</p>"},{"location":"api_docs/Predictor/#adaXT.predictor.predictor.PredictorClassification","title":"PredictorClassification","text":"<pre><code>PredictorClassification(X, Y, root)\n</code></pre> <p>The default prediction class for the 'Classification' tree type.</p>"},{"location":"api_docs/Predictor/#adaXT.predictor.predictor.PredictorClassification.predict","title":"predict","text":"<pre><code>predict(X, **kwargs)\n</code></pre> <p>For each row in X, the method first predicts the LeafNode into which the row falls and then computes the class with the highest number of occurances in that LeafNode.</p> <p>If the keyword argument 'predict_proba' is set to True. This method outputs class probabilities (i.e., the frequence of each label in the same LeafNode).</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>A 2-dimensional array for which the rows are the samples at which to predict.</p> required <code>**kwargs</code> <p>predict_proba : bool     Specifies whether to compute class probabilities or not.     Defaults to False if not provided.</p> <code>{}</code>"},{"location":"api_docs/Predictor/#adaXT.predictor.predictor.PredictorRegression","title":"PredictorRegression","text":"<pre><code>PredictorRegression(X, Y, root)\n</code></pre> <p>The default prediction class for the 'Regression' tree type.</p>"},{"location":"api_docs/Predictor/#adaXT.predictor.predictor.PredictorRegression.predict","title":"predict","text":"<pre><code>predict(X, **kwargs)\n</code></pre> <p>For each row in X, the method first predicts the LeafNode into which the row falls and then computes average response value in that LeafNode.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>A 2-dimensional array for which the rows are the samples at which to predict.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>An array with predictions for each row of X.</p>"},{"location":"api_docs/Predictor/#adaXT.predictor.predictor.PredictorLocalPolynomial","title":"PredictorLocalPolynomial","text":"<pre><code>PredictorLocalPolynomial(X, Y, root)\n</code></pre> <p>The default prediction class for the 'Gradient' tree type.</p>"},{"location":"api_docs/Predictor/#adaXT.predictor.predictor.PredictorLocalPolynomial.predict","title":"predict","text":"<pre><code>predict(X, **kwargs)\n</code></pre> <p>For each row in X, the method first predicts the LeafNode into which the row falls and then computes uses the parameters theta0, theta1 and theta2 saved on the LeafNode to estimate the predicted value $$ \\texttt{theta0} + \\texttt{theta1} \\texttt{X}[i, 0] + \\texttt{theta2} \\texttt{X}[i, 0]^2. $$</p> <p>Note: This predict class requires that the decision tree/random forest uses a LocalPolynomialLeafNode and either LeafBuilderPartialLinear or LeafBuilderPartialQuadratic.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>A 2-dimensional array for which the rows are the samples at which to predict.</p> required <code>**kwargs</code> <p>orders : list[int] | int     A list of integers or a single integer specifying which orders should be predicted.     Orders 0, 1 and 2 correspond to the 0th, 1st and 2nd order derivative.     Default is orders=[0, 1, 2] if not provided.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A 2-dim array with the predicted values for each row of X and columns corresponding to the orders.</p>"},{"location":"api_docs/Predictor/#adaXT.predictor.predictor.PredictorQuantile","title":"PredictorQuantile","text":"<pre><code>PredictorQuantile(X, Y, root)\n</code></pre> <p>The default prediction class for the 'Quantile' tree type.</p>"},{"location":"api_docs/Predictor/#adaXT.predictor.predictor.PredictorQuantile.predict","title":"predict","text":"<pre><code>predict(X, **kwargs)\n</code></pre> <p>For each row in X, the method first predicts the LeafNode into which the row falls and then computes the quantiles in that leaf node.</p> <p>For random forests the quantiles across all trees are computed jointly.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>A 2-dimensional array for which the rows are the samples at which to predict.</p> required <code>**kwargs</code> <p>quantile : list[float] | float     A list of quantiles or a single quantile which should be predicted.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A 1- or 2-dim array with the predicted quantiles for each row of X and columns corresponding to different quantiles (if quantiles was a list).</p>"},{"location":"api_docs/RandomForest/","title":"RandomForest Class","text":"<p>This is the class used to construct a random forest. Random forests consist of multiple individual decision trees that are trained on subsets of the data and then combined via averaging. This can greatly improve the generalization performance by avoiding the tendency of decision trees to over fit to the training data. Since random forest learn individual trees many of the parameters and functionality in this class overlaps with the DecisionTree class.</p> <p>The RandomForest can be imported as follows:</p> <pre><code>from adaXT.random_forest import RandomForest\n</code></pre>"},{"location":"api_docs/RandomForest/#adaXT.random_forest.random_forest.RandomForest","title":"RandomForest","text":"<pre><code>RandomForest(forest_type, n_estimators=100, n_jobs=1, sampling='resampling', sampling_args=None, max_features=None, max_depth=sys.maxsize, impurity_tol=0.0, min_samples_split=1, min_samples_leaf=1, min_improvement=0.0, seed=None, criteria=None, leaf_builder=None, predictor=None, splitter=None)\n</code></pre> <p>Attributes:</p> Name Type Description <code>max_features</code> <code>int | float | Literal[\"sqrt\", \"log2\"] | None = None</code> <p>The number of features to consider when looking for a split.</p> <code>max_depth</code> <code>int</code> <p>The maximum depth of the tree.</p> <code>forest_type</code> <code>str</code> <p>The type of random forest, either  a string specifying a supported type (currently \"Regression\", \"Classification\", \"Quantile\" or \"Gradient\").</p> <code>n_estimators</code> <code>int</code> <p>The number of trees in the random forest.</p> <code>n_jobs</code> <code>int | tuple[int, int]</code> <p>The number of jobs used to fit and predict. If tuple, then different between the two</p> <code>sampling</code> <code>str | None</code> <p>Either resampling, honest_tree, honest_forest or None.</p> <code>sampling_args</code> <code>dict | None</code> <p>A parameter used to control the behavior of the sampling scheme. The following arguments are available:     'size': Either int or float used by all sampling schemes (default 1.0).         Specifies the number of samples drawn. If int it corresponds         to the number of random resamples. If float it corresponds to the relative         size with respect to the training sample size.     'replace': Bool used by all sampling schemes (default True).         If True resamples are drawn with replacement otherwise without replacement.     'split': Either int or float used by the honest splitting schemes (default 0.5).         Specifies how to divide the sample into fitting and prediction indices.         If int it corresponds to the size of the fitting indices, while the remaining indices are         used as prediction indices (truncated if value is too large). If float it         corresponds to the relative size of the fitting indices, while the remaining         indices are used as prediction indices (truncated if value is too large).     'OOB': Bool used by all sampling schemes (default False).         Computes the out of bag error given the data set.         If set to True, an attribute called oob will be defined after         fitting, which will have the out of bag error given by the         Criteria loss function. If None all parameters are set to their defaults.</p> <code>impurity_tol</code> <code>float</code> <p>The tolerance of impurity in a leaf node.</p> <code>min_samples_split</code> <code>int</code> <p>The minimum number of samples in a split.</p> <code>min_samples_leaf</code> <code>int</code> <p>The minimum number of samples in a leaf node.</p> <code>min_improvement</code> <code>float</code> <p>The minimum improvement gained from performing a split.</p> <p>Parameters:</p> Name Type Description Default <code>forest_type</code> <code>str</code> <p>The type of random forest, either  a string specifying a supported type (currently \"Regression\", \"Classification\", \"Quantile\" or \"Gradient\").</p> required <code>n_estimators</code> <code>int</code> <p>The number of trees in the random forest.</p> <code>100</code> <code>n_jobs</code> <code>int</code> <p>The number of processes used to fit, and predict for the forest, -1 uses all available proccesors.</p> <code>1</code> <code>sampling</code> <code>str | None</code> <p>Either resampling, honest_tree, honest_forest or None.</p> <code>'resampling'</code> <code>sampling_args</code> <code>dict | None</code> <p>A parameter used to control the behavior of the sampling scheme. The following arguments are available:     'size': Either int or float used by all sampling schemes (default 1.0).         Specifies the number of samples drawn. If int it corresponds         to the number of random resamples. If float it corresponds to the relative         size with respect to the training sample size.     'replace': Bool used by all sampling schemes (default True).         If True resamples are drawn with replacement otherwise without replacement.     'split': Either int or float used by the honest splitting schemes (default 0.5).         Specifies how to divide the sample into fitting and prediction indices.         If int it corresponds to the size of the fitting indices, while the remaining indices are         used as prediction indices (truncated if value is too large). If float it         corresponds to the relative size of the fitting indices, while the remaining         indices are used as prediction indices (truncated if value is too large). If None all parameters are set to their defaults.</p> <code>None</code> <code>max_features</code> <code>int | float | Literal[\"sqrt\", \"log2\"] | None = None</code> <p>The number of features to consider when looking for a split.</p> <code>None</code> <code>max_depth</code> <code>int</code> <p>The maximum depth of the tree.</p> <code>maxsize</code> <code>impurity_tol</code> <code>float</code> <p>The tolerance of impurity in a leaf node.</p> <code>0.0</code> <code>min_samples_split</code> <code>int</code> <p>The minimum number of samples in a split.</p> <code>1</code> <code>min_samples_leaf</code> <code>int</code> <p>The minimum number of samples in a leaf node.</p> <code>1</code> <code>min_improvement</code> <code>float</code> <p>The minimum improvement gained from performing a split.</p> <code>0.0</code> <code>seed</code> <code>int | None</code> <p>Seed used to reproduce a RandomForest</p> <code>None</code> <code>criteria</code> <code>Criteria</code> <p>The Criteria class to use, if None it defaults to the forest_type default.</p> <code>None</code> <code>leaf_builder</code> <code>LeafBuilder</code> <p>The LeafBuilder class to use, if None it defaults to the forest_type default.</p> <code>None</code> <code>predictor</code> <code>Predictor</code> <p>The Prediction class to use, if None it defaults to the forest_type default.</p> <code>None</code> <code>splitter</code> <code>Splitter | None</code> <p>The Splitter class to use, if None it defaults to the default Splitter class.</p> <code>None</code>"},{"location":"api_docs/RandomForest/#adaXT.random_forest.random_forest.RandomForest.fit","title":"fit","text":"<pre><code>fit(X, Y, sample_weight=None)\n</code></pre> <p>Fit the random forest with training data (X, Y).</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like object of dimension 2</code> <p>The feature values used for training. Internally it will be converted to np.ndarray with dtype=np.float64.</p> required <code>Y</code> <code>array-like object</code> <p>The response values used for training. Internally it will be converted to np.ndarray with dtype=np.float64.</p> required <code>sample_weight</code> <code>ndarray | None</code> <p>Sample weights. Currently not implemented.</p> <code>None</code>"},{"location":"api_docs/RandomForest/#adaXT.random_forest.random_forest.RandomForest.predict","title":"predict","text":"<pre><code>predict(X, **kwargs)\n</code></pre> <p>Predicts response values at X using fitted random forest.  The behavior of this function is determined by the Prediction class used in the decision tree. For currently existing tree types the corresponding behavior is as follows:</p> Classification: <p>Returns the class based on majority vote among the trees. In the case of tie, the lowest class with the maximum number of votes is returned.</p> Regression: <p>Returns the average response among all trees.</p> Quantile: <p>Returns the conditional quantile of the response, where the quantile is specified by passing a list of quantiles via the <code>quantile</code> parameter.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array-like object of dimension 2</code> <p>New samples at which to predict the response. Internally it will be converted to np.ndarray with dtype=np.float64.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>(N, K) numpy array with the prediction, where K depends on the Prediction class and is generally 1</p>"},{"location":"api_docs/RandomForest/#adaXT.random_forest.random_forest.RandomForest.predict_weights","title":"predict_weights","text":"<pre><code>predict_weights(X=None, scale=True)\n</code></pre> <p>Predicts a weight matrix Z, where Z_{i,j} indicates if X_i and X0_j are in the same leaf node, where X0 denotes the training data. If scaling is True, then the value is divided by the number of other training data in the leaf node and averaged over all the estimators of the tree. If scaling is None, it is neither row-wise scaled and is instead summed up over all estimators of the forest.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ArrayLike | None</code> <p>New samples to predict a weight. If None then X is treated as the training and or prediction data of size Nxd.</p> <code>None</code> <code>scale</code> <code>bool</code> <p>Whether to do row-wise scaling</p> <code>True</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A numpy array of shape MxN, wehre N denotes the number of rows of the training and or prediction data.</p>"},{"location":"api_docs/RandomForest/#adaXT.random_forest.random_forest.RandomForest.similarity","title":"similarity","text":"<pre><code>similarity(X0, X1)\n</code></pre> <p>Computes a similarity Z of size NxM, where each element Z_{i,j} is 1 if element X0_i and X1_j end up in the same leaf node. Z is the averaged over all the estimators of the forest.</p> <p>Parameters:</p> Name Type Description Default <code>X0</code> <code>ArrayLike</code> <p>Array corresponding to row elements of Z.</p> required <code>X1</code> <code>ArrayLike</code> <p>Array corresponding to column elements of Z.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>A NxM shaped np.ndarray.</p>"},{"location":"api_docs/Splitter/","title":"Splitter Class","text":""},{"location":"api_docs/Splitter/#adaXT.decision_tree.splitter.Splitter","title":"Splitter","text":"<pre><code>Splitter(X, Y, criteria)\n</code></pre> <p>Splitter class used to create splits of the data.</p>"},{"location":"api_docs/Splitter/#adaXT.decision_tree.splitter.Splitter.get_split","title":"get_split","text":"<pre><code>get_split(indices, feature_indices)\n</code></pre> Function that finds the best split of the dataset <p>Parameters:</p> Name Type Description Default <code>indices</code> <code>memoryview of NDArray</code> <p>Indices for which to find a split.</p> required <code>feature_indices</code> <code>memoryview of NDArray</code> <p>Features at which to consider splitting.</p> required <p>Returns:</p> Type Description <code>(list, double, int, double, double)</code> <p>Returns the best split of the dataset, with the values being: (1) a list containing the left and right indices, (2) the best threshold for doing the splits, (3) what feature to split on, (4) the best criteria score, and (5) the best impurity</p>"},{"location":"api_docs/tree_utils/","title":"Tree Utilities","text":"<p>adaXT implements several methods that can be used to analyze and visualize fitted decision trees.</p> <p>All methods are available in the decision tree module.</p> <pre><code>import adaXT.decision_tree.tree_utils\n</code></pre>"},{"location":"api_docs/tree_utils/#adaXT.decision_tree.tree_utils.plot_tree","title":"plot_tree","text":"<pre><code>plot_tree(tree, impurity=True, node_precision=2, impurity_precision=3, ax=None, fontsize=None, max_depth=None)\n</code></pre>"},{"location":"user_guide/creatingCriteria/","title":"Creating a custom criteria","text":"<p>In this section we explain how to create a custom criteria function by walking through the required steps. The Criteria class is implemented as a Cython extension types -- also known as a cdef class. While this ensures that the criteria evaluations are fast, it also means that a custom criteria needs to be written as an extension type. From a practical perspective this means that, we need to implement the custom criteria in a .pyx file, that then needs to be compiled before use.</p> <p>We start by explaining how to create the .pyx file.</p>"},{"location":"user_guide/creatingCriteria/#creating-a-pyx-file-and-implementing-criteria","title":"Creating a .pyx file and implementing criteria","text":"<p>In your working directory create a .pyx file (e.g., <code>my_custom_criteria.pyx</code>) in which you first import the Criteria \"super\" class and then define the new custom criteria class which inherits from the imported Criteria class. An example of the skeleton of such a file is given as below.</p> <pre><code>from adaXT.criteria cimport Criteria\n\ncdef class My_custom_criteria(Criteria):\n    # Your implementation here\n</code></pre> <p>The class type of Criteria is a cdef class, which works in much of the same fashion as a regular Python class, but with faster performance. To read more on cdef classes checkout Cython extension types.</p> <p>Next, for the new criteria to work with adaXT you have to implement the impurity method inherited from the Criteria class. The impurity method has to follow the type definition specified within criteria.pxd, which is as follows:</p> <pre><code>    cpdef double impurity(self, int[:] indices):\n</code></pre> <p>The variable <code>indices</code> refers to the sample indices for which the impurity value should be computed. To access the feature and response you can make use of <code>self.x</code> and <code>self.y</code>, respectively. More specifically, <code>self.x[indices]</code> and <code>self.y[indices]</code> are the feature and response samples for which the impurity needs to be computed. With this in place you should be able to implement almost any criteria function you can imagine. Keep in mind that the <code>impurity</code> method is used often (approximately \\(n\\log(n)\\) times). Therefore you should invest a bit of time in optimizing the function in order to avoid long fitting times. Further computational speed-ups can be achieved by implementing <code>proxy_improvement</code> and <code>update_proxy</code> methods in the criteria class. If these are not explicitly defined the code defaults to using the <code>impurity</code> method. Although we do not provide in depth examples of those functionalities here, feel free to look at criteria.pyx where the default criteria make use of both.</p> <p>Once you have finished defining your critera class and saved the .pyx file, you can compile the Cython code and use it as part of adaXT.</p>"},{"location":"user_guide/creatingCriteria/#compiling-and-using-the-custom-criteria","title":"Compiling and using the custom criteria","text":"<p>We discuss two specific approaches for using your custom criteria. More details on how to compile and use Cython code can be found here.</p> <ul> <li>Building a Cython module: This allows you to only compile the new criteria   class once and then import it as a regular Python module.</li> <li>Use pyximport to import the .pyx file: This avoids needing to build the Cython   code manually and instead compiles the code each time you run your Python   code.</li> </ul>"},{"location":"user_guide/creatingCriteria/#option-1-building-a-cython-module","title":"Option 1: Building a Cython module","text":"<p>The first option of using your custom criteria is to create a <code>setup.py</code> file in which you build a Cython module that you can then import in your Python code. For this approach create a new subfolder (e.g., <code>custom_criteria/</code>) in your working directory in which you copy your .pyx file (e.g., <code>my_custom_criteria.pyx</code>) together with an empty file called <code>__init__.py</code>. Then in your working directory create a file called <code>setup.py</code> containing the following code:</p> <pre><code>from setuptools import setup\nfrom Cython.Build import cythonize\n\nsetup(\n    name='Custom Criteria',\n    ext_modules=cythonize(\"my_custom_criteria.pyx\"),\n)\n</code></pre> <p>The Cython code can now be compiled with the command</p> <pre><code>python setup.py build_ext --inplace\n</code></pre> <p>Note that this command needs to be run in an environment in which adaXT and setuptools is installed. After the code is built the custom criteria can be imported as a regular Python module as follows:</p> <pre><code>from custom_criteria import my_custom_criteria\n</code></pre>"},{"location":"user_guide/creatingCriteria/#option-2-using-pyximport-to-import-the-pyx-file","title":"Option 2: Using pyximport to import the .pyx file","text":"<p>Alternatively, you can let Cython compile the code each time you run your Python script. For this approach create your .py file in which you fit a decision tree using the new custom criteria within the same folder where you created the .pyx. For this to work there are a few small things left to do. In order to not manually recompile the Cython file every time you make changes, you can automate the compilation within the Python file by telling Cython to comile the Cython source file, whenever you run the file. Assuming your criteria is in the .pyx file <code>my_custom_criteria.pyx</code> you would import it as follows:</p> <pre><code>import pyximport; pyximport.install()\nimport my_custom_criteria\n</code></pre> <p>Now you can access the new criteria within the Python file like you would do with any other criteria function. For example, you could fit a decision tree with your custom criteria class <code>My_custom_criteria</code> as follows:</p> <pre><code>from adaXT.decision_tree import DecisionTree\nimport numpy as np\n\nn = 100\nm = 4\n\nX = np.random.uniform(0, 100, (n, m))\nY = np.random.uniform(0, 10, n)\ntree = DecisionTree(\"Regression\", criteria=my_custom_critera.My_custom_criteria, max_depth=3)\ntree.fit(X, Y)\n</code></pre> <p>We now go over a detailed example in which we construct the <code>PartialLinear</code> criteria.</p>"},{"location":"user_guide/creatingCriteria/#a-detailed-example-partiallinear","title":"A detailed example: <code>PartialLinear</code>","text":"<p>The general idea of the <code>PartialLinear</code> criteria is to fit a linear function on the first feature with the \\(Y\\) value as the response, that is,</p> \\[ L = \\sum_{i \\in I} (Y[i] - \\theta_0 - \\theta_1 X[i, 0])^2 \\\\ \\] \\[ \\theta_1 = \\frac{\\sum_{i \\in I} (X[i, 0] - \\mu_X)(Y[i] - \\mu_Y)}{\\sum_{i \\in I} (X[i, 0] - \\mu_X)^2} \\\\ \\] \\[ \\theta_0 = \\mu_Y - \\theta_1 \\mu_X, \\] <p>where \\(I\\) denotes the indices of the samples within a given node, \\(X\\) is the feature matrix, \\(Y\\) is the response vector, \\(\\mu_X\\) is mean vector of the columns of \\(X\\) and \\(\\mu_Y\\) is the mean of \\(Y\\).</p> <p>When creating a new criteria class we import and define the class as described above. We open a new file in our working directory and call it <code>testCrit.pyx</code> and start with the following lines:</p> <pre><code>from adaXT.criteria cimport Criteria\n\ncdef class PartialLinear(Criteria):\n</code></pre>"},{"location":"user_guide/creatingCriteria/#calculating-the-mean","title":"Calculating the mean","text":"<p>Now although we have provided a mean function within the <code>adaXT.decision_tree.crit_helpers</code>, in this specific example we need to calculate multiple means, and there is no reason to do two passes over the same indices \\(I\\), so we create a custom mean method in Cython.</p> <pre><code># Custom mean function, such that we don't have to loop through twice.\ncdef (double, double) custom_mean(self, int[:] indices):\n    cdef:\n        double sumX, sumY\n        int i\n        int length = indices.shape[0]\n    sumX = 0.0\n    sumY = 0.0\n    for i in range(length):\n        sumX += self.x[indices[i], 0]\n        sumY += self.y[indices[i]]\n\n    return ((sumX / (&lt;double&gt; length)), (sumY / (&lt;double&gt; length)))\n</code></pre> <p>You might notice that the syntax is a little different than for default Python. In general you can write default Python within the cdef class, however the runtime can be significantly decreased by adding type definitions and other Cython magic. If you wish to learn more about the Cython language be sure to check out the Cython documentation.</p> <p>Furthermore, note that you can freely create any new method within the custom criteria class even if it is not defined in the standard parent criteria class. Therefore you are not limited by the parent class and can freely extend it as long as you do not overwrite the <code>evaluate_split</code> (unless that is your intention).</p>"},{"location":"user_guide/creatingCriteria/#calculating-theta","title":"Calculating theta","text":"<p>Now that we have a method for getting the mean, we can create a method for calculating the theta values, such that our code is more manageable.</p> <pre><code>cdef (double, double) theta(self, int[:] indices):\n    \"\"\"\n    Estimates regression parameters for a linear regression of the response on\n    the first coordinate, i.e., Y is approximated by theta0 + theta1 * X[:, 0].\n    ----------\n\n    Parameters\n    ----------\n    indices : memoryview of NDArray\n        The indices to calculate\n\n    Returns\n    -------\n    (double, double)\n        where first element is theta0 and second is theta1\n    \"\"\"\n    cdef:\n        double muX, muY, theta0, theta1\n        int length, i\n        double numerator, denominator\n        double X_diff\n\n    length = indices.shape[0]\n    denominator = 0.0\n    numerator = 0.0\n    muX, muY = self.custom_mean(indices)\n    for i in range(length):\n        X_diff = self.x[indices[i], 0] - muX\n        numerator += (X_diff)*(self.y[indices[i]]-muY)\n        denominator += (X_diff)*X_diff\n    if denominator == 0.0:\n        theta1 = 0.0\n    else:\n        theta1 = numerator / denominator\n    theta0 = muY - theta1*muX\n    return (theta0, theta1)\n</code></pre> <p>Again the majority of the Cython is not mandatory but speeds up the code. On line 26 we access our previously defined custom mean function, which returns the mean of the \\(X\\) indices and the mean of the \\(Y\\) indices as described above. Then on line 27 we loop over all the indices a second time and calculate \\(\\sum_{i \\in I} (X[i, 0] - \\mu_X) (Y[i] - \\mu_Y)\\) and \\(\\sum_{i \\in I} (X[i, 0] - \\mu_X)^2\\) which are the numerator and denominator, respectively. These are the two values used to calculate \\(\\theta_1\\). Further on line 31 we check to make sure that the denominator is not 0.0, this ensures that we can consider the underidentified case separately and do not divide by zero by accident. If the denominator is zero, we simply set \\(\\theta_1\\) to 0.0 as this will give an L value of 0 in the end. We finish off by returning the two values \\(\\theta_0,\\theta_1\\).</p>"},{"location":"user_guide/creatingCriteria/#the-impurity-function","title":"The impurity function","text":"<p>Finally, we reach the most important step, which is to create the impurity function.</p> <pre><code>cpdef double impurity(self, int[:] indices):\n    cdef:\n        double step_calc, theta0, theta1, cur_sum\n        int i, length\n\n    length = indices.shape[0]\n    theta0, theta1 = self.theta(indices)\n    cur_sum = 0.0\n    for i in range(length):\n        step_calc = self.y[indices[i]] - theta0 - theta1 * self.x[indices[i], 0]\n        cur_sum += step_calc*step_calc\n    return cur_sum\n</code></pre> <p>Making sure the impurity method has the required signature, we simply calculate L as described and return its value. One important point to note is that <code>cur_sum</code> is defined on line 3 to have type double, because the impurity function is defined to have a double as return value. When creating the impurity method you must ensure this return type.</p>"},{"location":"user_guide/creatingCriteria/#finishing-up","title":"Finishing up","text":"<p>And that is it. You have now created your first custom criteria class, and can freely use it within your own Python code. If you use the method that compiles the Cython file every time the Python file is run leads to code that looks like this:</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom adaXT.decision_tree import DecisionTree\nfrom adaXT.decision_tree.tree_utils import plot_tree\n\nimport pyximport\npyximport.install()\nimport testCrit\n\n# Generate training data\nn = 100\nm = 4\nX = np.random.uniform(0, 100, (n, m))\nY = np.random.uniform(0, 10, n)\n\n# Initialize and fit tree\ntree = DecisionTree(\"Regression\", testCrit.PartialLinear, max_depth=3)\ntree.fit(X, Y)\n\n# Plot the tree\nplot_tree(tree)\nplt.show()\n</code></pre> <p>This creates a regression tree with the newly created custom <code>PartialLinear</code> criteria class, specifies the <code>max_depth</code> to be 3 and then plots the tree using both the plot_tree based on matplotlib. The full source code used within this article can be found here.</p>"},{"location":"user_guide/creatingPredictor/","title":"Creating a custom Predictor","text":""},{"location":"user_guide/creatingPredictor/#general-overview-of-the-predictor","title":"General overview of the Predictor","text":"<p>Like other elements in adaXT, it is possible to create a custom Predictor. You can start by creating a new .pyx file using the following template:</p> <pre><code>from adaXT.predictor cimport Predictor\n\ncdef class MyPredictorClass(Predictor):\n\n  cdef:\n    # attribute_type attribute_name\n\n  def __init__(\n    self,\n    double[:, ::1] X,\n    double[:, ::1] Y,\n    object root,\n    **kwargs):\n  super().__init__(X, Y, root, **kwargs)\n  # Any custom initialization you would need for your predictor class\n  # If you don't have any, you don't need to define the __init__ function.\n\n\n  def predict(self, cnp.ndarray X, **kwargs) -&gt; np.ndarray: \n    # Define your own custom predict function\n\n  @staticmethod\n  def forest_predict(cnp.ndarray X_train, cnp.ndarray Y_train, cnp.ndarray X_pred,\n                      trees: list[DecisionTree], parallel: ParallelModel,\n                      **kwargs) -&gt; np.ndarray:\n    # Define special handling for the RandomForest predict.\n    # If it is not defined, then the RandomForest will take the mean of all the\n    # predict for its estimators.\n</code></pre> <p>The template includes three main components:</p> <ol> <li>__init__ function: This function is used to initialize the class. Because Cython    removes a lot of the boilerplate with default Python classes Cython, you cannot    add attributes to a cdef class without explicitly defining them. The __init__    function allows you to initialize these attributes after you have defined them above.    If you do not need additional attributes, you can skip this step.</li> <li>predict method: This method is used to compute predictions for the given input X    values. It is a standard Python method and can be used like any other. Within this    method, you have access to the general attributes of the    Predictor class, including the number of features and    the root node object, which can be used to traverse the tree.</li> <li>forest_predict method: This static method aggregates predictions across multiple    trees for forest predictions. It enables parallel processing across trees. If your    custom Predictor simply averages tree predictions, you can inherit this method    from the base Predictor class.</li> </ol>"},{"location":"user_guide/creatingPredictor/#example-of-creating-a-predictor","title":"Example of creating a Predictor","text":"<p>To illustrate each component, we go over the PredictorQuantile class, which is used for quantile regression trees and forests. It does not add any additional attributes so the __init__ function is not needed in this case.</p>"},{"location":"user_guide/creatingPredictor/#the-predict-method","title":"The predict method","text":"<p>In quantile regression we want to predict the quantiles of the conditional distribution instead of just the conditional mean as in regular regression. For a single tree this can be done with the following predict method:</p> <pre><code>cdef class PredictorQuantile(Predictor):\n    def predict(self, cnp.ndarray X, **kwargs) -&gt; np.ndarray:\n        cdef:\n            int i, cur_split_idx, n_obs\n            double cur_threshold\n            object cur_node\n            cnp.ndarray prediction\n        if \"quantile\" not in kwargs.keys():\n            raise ValueError(\n                        \"quantile called without quantile passed as argument\"\n                    )\n        quantile = kwargs['quantile']\n        # Make sure that x fits the dimensions.\n        n_obs = X.shape[0]\n        # Check if quantile is an array\n        if isinstance(quantile, Sequence):\n            prediction = np.empty((n_obs, len(quantile)), dtype=DOUBLE)\n        else:\n            prediction = np.empty(n_obs, dtype=DOUBLE)\n\n        for i in range(n_obs):\n            cur_node = self.root\n            while isinstance(cur_node, DecisionNode):\n                cur_split_idx = cur_node.split_idx\n                cur_threshold = cur_node.threshold\n                if X[i, cur_split_idx] &lt; cur_threshold:\n                    cur_node = cur_node.left_child\n                else:\n                    cur_node = cur_node.right_child\n\n            prediction[i] = np.quantile(self.Y.base[cur_node.indices, 0], quantile)\n        return prediction\n</code></pre> <p>Here, we first define the types of the variables used. This allows Cython to optimize the code, which leads to a faster prediction runtime.</p> <p>Next, we check the kwargs for the key <code>quantile</code>. Any keyword arguments passed to the DecisionTree.predict is passed directly to the Predictor.predict, meaning that we can access the desired quantile from the predict signature without having to change anything else. As we want to allow for multiple quantiles to be predicted at the same time, we have to initalize the <code>prediction</code> variable differently depending on whether <code>quantile</code> is a Sequence or just a single element.</p> <p>Finally, we iterate over the tree: For every observation, we go to the root node and loop as long as we are in a DecisionNode. In each step, we check if we split to the left or the right, and traverse down the tree. Once <code>cur_node</code> is no longer an instance of the DecisionNode, we know that we have reached a LeafNode. We can access all Y values via <code>self.Y.base</code> ('.base' has to be added, as we are indexing with a list of elements) and the indices of the elements within the LeafNode via <code>cur_node.indices</code>. As we only have a single Y output value, we simply want the first column of Y. This is then repeated for the rest of the provided X values.</p>"},{"location":"user_guide/creatingPredictor/#the-forest_predict-method","title":"The forest_predict method","text":"<p>The forest_predict method looks a lot more intimidating, but is just as straightforward as the predict method. Here is the code:</p> <pre><code>def predict_quantile(\n    tree: DecisionTree, X: np.ndarray, n_obs: int\n) -&gt; list:\n    # Check if quantile is an array\n    indices = []\n\n    for i in range(n_obs):\n        cur_node = tree.root\n        while isinstance(cur_node, DecisionNode):\n            cur_split_idx = cur_node.split_idx\n            cur_threshold = cur_node.threshold\n            if X[i, cur_split_idx] &lt; cur_threshold:\n                cur_node = cur_node.left_child\n            else:\n                cur_node = cur_node.right_child\n\n        indices.append(cur_node.indices)\n    return indices\n\ncdef class PredictorQuantile(Predictor):\n  @staticmethod\n  def forest_predict(cnp.ndarray X_train, cnp.ndarray Y_train, cnp.ndarray X_pred,\n                      trees: list[DecisionTree], parallel: ParallelModel,\n                      **kwargs) -&gt; np.ndarray:\n      cdef:\n          int i, j, n_obs, n_trees\n          list prediction_indices, pred_indices_combined, indices_combined\n      if \"quantile\" not in kwargs.keys():\n          raise ValueError(\n              \"quantile called without quantile passed as argument\"\n          )\n      quantile = kwargs['quantile']\n      n_obs = X_pred.shape[0]\n      prediction_indices = parallel.async_map(predict_quantile,\n                                              map_input=trees, X=X_pred,\n                                              n_obs=n_obs)\n      # In case the leaf nodes have multiple elements and not just one, we\n      # have to combine them together\n      n_trees = len(prediction_indices)\n      pred_indices_combined = []\n      for i in range(n_obs):\n          indices_combined = []\n          for j in range(n_trees):\n              indices_combined.extend(prediction_indices[j][i])\n          pred_indices_combined.append(indices_combined)\n      ret = np.quantile(Y_train[pred_indices_combined], quantile)\n      return np.array(ret, dtype=DOUBLE)\n</code></pre> <p>The forest_predict method is a staticmethod, meaning that it is tied to the Predictor class itself and not a specific instance of the class. The reason for this is that it allows us to fully control the parallization over trees. For the PredictorQuantile, for example, we want to be able to control this ourselves.</p> <p>As before we define the variables used and check the input for the kwarg <code>quantile</code>. However, this time we needed to define a globally available function <code>predict_quantile</code> at the top level of the file. It has to be a globally available for the multiprocessing to work probably. This function traverses a given tree, and finds the LeafNode each element of X would end up in and adds the indices of the elements already in the LeafNode. We then call <code>predict_quantile</code> using the parallel.async_map, which is adaXTs way of making parallelization more manageable. It makes use of the Parallel class. The async_map calls <code>predict_quantile</code> with all the trees in parallel, and returns the result. This means, that <code>prediction_indices</code> will contain a list with the length equal to the number of trees in the forest. Each element of the list will be a single trees prediction for the input array X. We then create a list <code>pred_indices_combined</code> where we combine all the predictions for X. To get the final result, we then just call numpy's quantile implementation.</p>"},{"location":"user_guide/decision_tree/","title":"Decision Trees","text":"<p>A decision tree is a machine learning model, which is fitted to data and then used for prediction or data analysis. Decisions trees have a tree structure, where each internal node splits the dataset based on a threshold value for a given feature index.</p> <p>The DecisionTree class is used to create decision trees in adaXT. On an abstract level a decision tree defines two procedures:</p> <ol> <li>Fit: Given training data, create a list of nodes arranged in a tree    structure consisting of three types of nodes: (i) A root node, (ii) decision    nodes and (iii) leaf nodes. In adaXT the fit procedure is determined by the    <code>criteria</code>, <code>leaf_builder</code> and <code>splitter</code> parameters, as well as several    other hyperparameters that are common across all decision trees.</li> <li>Predict: Given test data, create predictions for all test samples by    propagating them through the tree structure and using the leaf node they land    in to create a prediction. In adaXT the predict procedure is determined by    the <code>predict</code> parameter.</li> </ol> <p>For a given application, one needs to fully specify these two procedures. In adaXT, this can either be done by specifying an existing default <code>tree_type</code> or by directly specifying all components manually.</p>"},{"location":"user_guide/decision_tree/#tree-types","title":"Tree types","text":"<p>There are several default tree types implemented in adaXT.</p> <ul> <li><code>Classification</code>: For prediction tasks in which the response is categorical.</li> <li><code>Regression</code>: For prediction tasks in which the response is continuous.</li> <li><code>Quantile</code>: For uncertainty quantification tasks in which the response is   continuous and the goal is to estimate one or more quantiles of the   conditional distribution of the response given the predictors.</li> <li><code>Gradient</code>: For tasks in which one aims to estimate (directional) derivatives   of the response given the predictors. A related tree type is used in the   Xtrapolation   method.</li> </ul> <p>The defaults for each of these tree types is set in the <code>BaseModel</code> class, which is extended by both the <code>DecisionTree</code> and <code>RandomForest</code> classes. Moreover, if you want to create a custom tree type, this can be done by setting the tree type to <code>None</code> and providing all components manually. Each of these options is discussed in the following sections.</p>"},{"location":"user_guide/decision_tree/#classification-trees","title":"Classification trees","text":"<p>For the <code>Classification</code> tree type, the following default components are used:</p> <ul> <li>Criteria class:   Entropy</li> <li>Predict class:   PredictorClassification</li> <li>LeafBuilder class:   LeafBuilderClassification</li> </ul> <p>Below is a short example that illustrates how to use a classification tree.</p> <pre><code>import numpy as np\nfrom adaXT.decision_tree import DecisionTree\nfrom adaXT.criteria import GiniIndex\n\nX = np.array([[1, 1], [1, -1], [-1, -1], [-1, 1],\n              [1, 1], [1, -1], [-1, -1], [-1, 1]])\nXtest = np.array([[1, 1], [1, -1], [-1, -1], [-1, 1]])\nY = [0, 1, 0, 1, 0, 0, 1, 1]\n\ntree = DecisionTree(\"Classification\", criteria=GiniIndex)\ntree.fit(X, Y)\nprint(tree.predict(Xtest))\nprint(tree.predict(Xtest, predict_proba=True))\n</code></pre> <p>In this example we created and fit a classification tree using training data and then used the fitted tree to predict the response at the training data. When initializing the tree we changed the default criteria to the Gini Index; it is always possible to overwrite any of the default components of a specific tree type. Classification trees use a majority vote in each of the leaf nodes to decide which class to predict and ties are broken by selecting the smaller class. To predict the class probabilities instead of the class labels, one can add <code>predict_proba=True</code> as a keyword argument.</p> <p>For classification trees it is also possible, using the <code>predict_proba</code> method, to output the proportions of each class instead of only the majority class. That method returns an array with the probability of an element being either of the classes. To get the list of classes (in the correct order), one can use <code>.classes</code> attribute of a fitted decision tree.</p> <p>Note that in this example, the decision tree was too constrained to fit the data, if one chooses the <code>max_depth</code> parameter larger the tree can perfectly fit the data.</p>"},{"location":"user_guide/decision_tree/#regression-trees","title":"Regression trees","text":"<p>For the <code>Regression</code> tree type, the following default components are used:</p> <ul> <li>Criteria class:   SquaredError</li> <li>Predict class:   PredictRegression</li> <li>LeafBuilder class:   LeafBuilderRegression</li> </ul> <p>Regression trees work similar to classification trees as illustrated in the following example:</p> <pre><code>import numpy as np\nfrom adaXT.decision_tree import DecisionTree\n\nn = 100\nX = np.random.normal(0, 1, (n, 2))\nY = 2.0 * (X[:, 0] &gt; 0) + np.random.normal(0, 0.25, n)\nXnew = np.array([[1, 0], [-1, 0]])\n\ntree = DecisionTree(\"Regression\", min_samples_leaf=20)\ntree.fit(X, Y)\nprint(tree.predict(Xnew))\n</code></pre>"},{"location":"user_guide/decision_tree/#quantile-trees","title":"Quantile trees","text":"<p>For the <code>Quantile</code> tree type, the following default components are used:</p> <ul> <li>Criteria class:   SquaredError</li> <li>Predict class:   PredictorQuantile</li> <li>LeafBuilder class:   LeafBuilderRegression</li> </ul> <p>Quantile trees are the building block for quantile random forests that were proposed by Meinshausen, 2006. They have the same interface as regression and classification trees, but the predict method takes the additional mandatory keyword <code>quantile</code> which specifies which quantiles to estimate. The following example illustrates this:</p> <pre><code>import numpy as np\nfrom adaXT.decision_tree import DecisionTree\n\nn = 100\nX = np.random.normal(0, 1, (n, 2))\nY = 10.0 * (X[:, 0] &gt; 0) + np.random.normal(0, 1, n)\nXnew = np.array([[1, 0], [-1, 0]])\n\ntree = DecisionTree(\"Quantile\", min_samples_leaf=20)\ntree.fit(X, Y)\nprint(tree.predict(Xnew, quantile=[0.1, 0.5, 0.9]))\n</code></pre> <p>As seen from this example, the quantiles do not need to be specified prior to prediction and it is possible to predict several quantiles simultaneously.</p>"},{"location":"user_guide/decision_tree/#gradient-trees","title":"Gradient trees","text":"<p>For the <code>Gradient</code> tree type, the following default components are used:</p> <ul> <li>Criteria class:   PartialQuadratic</li> <li>Predict class:   PredictLocalPolynomial</li> <li>LeafBuilder class:   LeafBuilderLocalPolynomial</li> </ul> <p>Gradient trees are a non-standard type of trees that allow estimation of derivates (in the first coordinate) of the conditional expectation function. The provided implementation is a slight modification of the procedure used in the Xtrapolation method. Instead of using the raw response values, we generally recommend to first use your favorite regression procedure (e.g., a regular random forest or a neural network) and use the resulting fitted values instead of the raw response. This has the advantage of first removing noise and then using the gradient tree as an additional step to estimate the gradients.</p> <p>Gradient trees are similar to regression trees but instead of fitting a constant in each leaf they fit a quadratic function in the first predictor variable \\(X[:, 0]\\). This allows them to fit quadratic functions in the first coordinate without splitting, as illustrated in the following example:</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom adaXT.decision_tree import DecisionTree\n\nn = 200\nX = np.random.normal(0, 1, (n, 1))\nY = X[:, 0] ** 2 + np.random.normal(0, 0.5, n)\n\ntree_reg = DecisionTree(\"Regression\", min_samples_leaf=50)\ntree_reg.fit(X, Y)\nYhat_reg = tree_reg.predict(X)\n\ntree_grad = DecisionTree(\"Gradient\", min_samples_leaf=50)\ntree_grad.fit(X, Y)\nYhat_grad = tree_grad.predict(X,  order=[0, 1])\n\nplt.scatter(X, Y, label='raw data')\nplt.scatter(X, Yhat_reg, label='Regression tree')\nplt.scatter(X, Yhat_grad[:, 0], label='Gradient tree (order 0)')\nplt.scatter(X, Yhat_grad[:, 1], label='Gradient tree (order 1)')\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"user_guide/decision_tree/#custom-tree-types","title":"Custom tree types","text":"<p>It is also possible to manually specify the tree type. This is particularly useful when you have custom components for the tree and do not want to use any of the default classes. To do this simply set <code>tree_type</code> to None and provide the <code>criteria</code>, <code>predictor</code> and <code>leaf_builder</code> classes when initializing the tree.</p>"},{"location":"user_guide/decision_tree/#further-functionality","title":"Further functionality","text":"<p>adaXT provides various additional functionality, each of which is discussed in other sections of the user guide.</p> <ul> <li>Tree-based weights: A fitted   decision tree provides a similarity notion on the predictor space that has   some useful properties. Check out this section to see how this can be used.</li> <li>Visualizations and analysis tools: There are   several function available that can help with analyzing a fitted decision   tree.</li> </ul>"},{"location":"user_guide/honest_splitting/","title":"Honest splitting","text":"<p>Decision trees generally use the training data twice during fitting: Once for deciding where to create splits resulting in the tree structure and once to create a prediction for each leaf node.</p> <p>While such a double use of the data may lead to overfitting, the effect is often negligible, in particular when otherwise regularizing (e.g., fitting a forest or constraining the maximum depth). Nevertheless it can be beneficial to adapt the splitting to improve generalization performance. One way of achieving this is via honest splitting (Athey and Imbens, 2016). The approach was originally developed in the context of causal effect estimation, where the bias is shown to be reduced by honest splitting allowing for inference on the causal effects.</p> <p>Below we provide a short overview of honest splitting and explain how to use it in adaXT.</p>"},{"location":"user_guide/honest_splitting/#honest-splitting-in-adaxt","title":"Honest splitting in adaXT","text":"<p>In its most basic form honest splitting consists of dividing the training data into two disjoint subsets; a fitting set, called <code>fitting_indices</code> in the code, used to create a fitted tree and a prediction set, called <code>prediction_indices</code> in the code, used to populate the leaf nodes that are later used when predicting.</p> <p>This basic version of honest splitting can be performed manually in adaXT by using the <code>refit_leaf_nodes</code> function as follows:</p> <pre><code>from adaXT.decision_tree import DecisionTree\nimport numpy as np\n\n# Create toy training data\nn = 100\nX = np.random.normal(0, 1, (n, 2))\nY = 2.0 * (X[:, 0] &gt; 0) + np.random.normal(0, 0.25, n)\n\n# Split training data\nfitting_indices = np.arange(0, int(n/2))\nprediction_indices = np.arange(int(n/2)+1, n)\n\n# Fit a regression decision tree using only fitting_indices\ntree = DecisionTree(\"Regression\", max_depth=5)\ntree.fit(X, Y, sample_indices=fitting_indices)\n\n# Predict on two test points\nprint(tree.predict(np.array([[-1, 0], [1, 0]])))\n\n# Refit tree using only prediction_indices\ntree.refit_leaf_nodes(X, Y, sample_indices=prediction_indices)\n\n# Predict on the same two test points\nprint(tree.predict(np.array([[-1, 0], [1, 0]])))\n</code></pre> <p>Using the <code>refit_leaf_nodes</code> function directly is tedious when fitting random forests. Therefore the RandomForest class has an optional parameter to perform honest splitting. The precise behaviour is controlled by the parameters <code>sampling</code> and <code>sampling_args</code>. Currently there are two version of honest splitting available:</p> <ul> <li>honest_tree: In this case, for each tree in the forest, a new split into   <code>fitting_indices</code> and <code>prediction_indices</code> is randomly drawn and used to fit   and refit the tree, respectively. Unlike in a classical random forest that   draws a bootstrap sample, this approaches first for each tree randomly divides   the training data into two parts (controlled by <code>sampling_args['split']</code>) and   then from each draws a separate random sample (controlled by   <code>sampling_args['size']</code> and <code>sampling_args['replace']</code>). The first subsample   (the <code>fitting_indices</code>) is then used to create splits and the second (the   <code>prediction_indices</code>) to populate the leaf nodes.</li> <li>honest_forest: In this case, the data is split only once into two parts   (controlled by <code>sampling_args['split']</code>) instead of randomly for each tree   (note that the split is done without permuting, which means that order in the   data may affect results). For each tree a random subsample is drawn for each   part (the <code>fitting_indices</code> and the <code>prediction_indices</code>). Again this   resampling is controlled by <code>sampling_args['size']</code> and   <code>sampling_args['replace']</code>. This approach ensures a total separation between   the data used to create the splits and the data used to populate the leafs.   Importantly, this guarantees (for independent training samples) that the   resulting random forest weights (extracted using <code>predict_weights</code>) are   independent of the samples corresponding to the <code>fitting_indices</code>.</li> </ul>"},{"location":"user_guide/installation/","title":"Installation","text":"<p>We recommend installing adaXT via pypi using pip. This can be done using the following command:</p> <pre><code>pip install adaXT\n</code></pre> <p>Alternatively, it can also be installed directly from the github repository:</p> <pre><code>pip install git+https://github.com/NiklasPfister/adaXT.git#egg=adaXT\n</code></pre> <p>Some new features might not yet be merged onto the main branch. If you are feeling experimental and want to try out the current development version you can install it with the following command:</p> <pre><code>pip install git+https://github.com/NiklasPfister/adaXT.git@Development#egg=adaXT\n</code></pre>"},{"location":"user_guide/installation/#modifying-the-project-and-building-it-locally","title":"Modifying the project and building it locally","text":"<p>Simple extensions such as adding a custom criteria or predictor class can be easily done without any modifications to the base package, as described here and here. However, more involved changes may require changing some of the inner workings of the package. As it is one of the main goals of adaXT to provide an adaptable and extendable package, we have tried to make such changes as easy as possible by keeping the code as simple as possible.</p> <p>If you want to modify the package itself, you can follow the following steps to download the project and then build it locally.</p> <ol> <li>Download source code: Either you directly download the repository github    or you    create a fork on github.</li> <li>Modify code: Modify or extend the code as you please.</li> <li>Build and install package: From the project root directory you can then    build and install the package with the command:</li> </ol> <pre><code>pip install .\n</code></pre> <p>This will require the    setuptools package to be    installed. Note that if you added new files or directories you will also need    to modify the <code>setup.py</code> file accordingly. 4. Use package: Once the package is installed you can use it in the same way    in which you used the original package.</p> <p>You can also consider creating a pull request, if you think your improvement    or extension could be of interest to others.</p>"},{"location":"user_guide/overview_components/","title":"Overview of components in adaXT","text":""},{"location":"user_guide/overview_components/#general-structure","title":"General structure","text":""},{"location":"user_guide/overview_components/#replaceable-components","title":"Replaceable components","text":""},{"location":"user_guide/overview_components/#splitter-class","title":"Splitter class","text":""},{"location":"user_guide/overview_components/#criteria-class","title":"Criteria class","text":""},{"location":"user_guide/overview_components/#leaf-builder-class","title":"Leaf builder class","text":""},{"location":"user_guide/overview_components/#prediction-class","title":"Prediction class","text":""},{"location":"user_guide/random_forest/","title":"Random Forests","text":"<p>Random forests are ensembles of decision trees that aggregate the predictions of all individual trees. By combining multiple decision trees, each trained on slightly different training data, random forests naturally avoid overfitting and tend to generalize much better than decision trees alone.</p> <p>The RandomForest class is used in adaXT to create random forests. It takes mostly the same parameters as the DecisionTree class, as illustrated in the example below.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom adaXT.random_forest import RandomForest\nfrom adaXT.criteria import PartialLinear\nfrom adaXT.leaf_builder import LeafBuilderPartialLinear\nfrom adaXT.predictor import PredictorLocalPolynomial\n\n# Training and test data\nn = 200\nXtrain = np.random.uniform(-1, 1, (n, 1))\nYtrain = np.sin(Xtrain[:, 0]*np.pi) + np.random.normal(0, 0.2, n)\nXtest = np.linspace(-1, 1, 50).reshape(-1, 1)\n\n# Fit a regular regression forest and a regression forest with linear splits\nrf = RandomForest(\"Regression\", min_samples_leaf=30)\nrf_lin = RandomForest(\"Regression\",\n                      criteria=PartialLinear,\n                      leaf_builder=LeafBuilderPartialLinear,\n                      predictor=PredictorLocalPolynomial,\n                      min_samples_leaf=30)\nrf.fit(Xtrain, Ytrain)\nrf_lin.fit(Xtrain, Ytrain)\n\n# Plot the resulting fits on Xtest\nplt.scatter(Xtrain, Ytrain, alpha=0.5)\nplt.plot(Xtest, rf.predict(Xtest), label=\"regular RF\")\nplt.plot(Xtest, rf_lin.predict(Xtest, order=0), label=\"linear RF\")\nplt.legend()\nplt.show()\n</code></pre> <p>In this example, we fit a regular regression forest (which uses the SquaredError) and a regression forest that uses the PartialLinear splitting criteria and predicts a linear function in each leaf. As can be seen when running this example, the forest with the linear splits is able to produce a better fit when both forests are grown similarly deep.</p>"},{"location":"user_guide/random_forest/#implementation-details","title":"Implementation details","text":"<p>The RandomForest class has essentially two functionalities. Firstly, it implements several sampling schemes that split the data and secondly it handles initializing and fitting of all of the DecisionTrees based on these splits.</p>"},{"location":"user_guide/random_forest/#sampling-schemes","title":"Sampling schemes","text":"<p>In the basic random forest as proposed by Breiman, 2001, one draws for a each decision tree a new random bootstrap sample from the full the training data (that is, with replacement) and uses only the boostrap data to fit the decision tree. The idea behind this subsampling is that while each individual tree may overfit to its training sample, the overfitting averages out as each tree overfits to a different data set.</p> <p>While the original bootstrap sampling scheme is the most common approach, other sampling schemes have also been proposed and the choice can have a significant impact on the generalization performance of the random forest. adaXT currently implements the following sampling schemes that are selected via the <code>sampling</code> parameter with more refined settings available via the <code>sampling_parameters</code> parameter:</p> <ul> <li><code>bootstrap</code>: Subsamples are drawn with replacement from the full training   sample. The size of the subsamples can be controlled with the   <code>sampling_parameters</code>.</li> <li><code>honest_tree</code>: This is a weak form of honest splitting   which splits the training data for each tree into two and uses one split to   fit the tree structure and the other split to populate the leafs.</li> <li><code>honest_forest</code>: This is a strong form of   honest splitting which splits the training data once   and then uses one split with bootstrapping to fit the tree structures and the   other part to populate the leafs of all trees.</li> <li><code>None</code>: The entire training sample is used to fit each tree. This is generally   not recommended as long as the decision trees are deterministic.</li> </ul>"},{"location":"user_guide/random_forest/#parallelization-via-multiprocessing","title":"Parallelization via multiprocessing","text":"<p>Since each decision tree can be fitted separately, it is possible to parallelize the fitting of a random forest. In adaXT we chose to use multiprocessing, using Python's built-in multiprocessing library, instead of multithreading. The advantage of this approach is that it allows users to define their own criteria functions without requiring the Global Interpreter Lock (GIL) to be released. However, it adds an additional overhead cost related to managing the individual processes. Consequently, there is an inherent trade-off between the setup time for additional processes and the workload allocated to each individual process. For smaller datasets or models with fewer trees, this often leads to diminishing returns as more processors are utilized. Users are therefore encouraged to select the <code>n_jobs</code> parameter with this trade-off in mind.</p> <p>In order to avoid making the random forest code too complex, we have separated the multiprocessing logic into a separate class called ParallelModel. The ParallelModel offers a variety of methods capable of computing functions in parallel. With this it aims to reduce the complexity of working with multiprocessing.</p> <p>When working with the ParallelModel we generally advise on creating the parallel functions on the module level instead of being class methods. Class method parallelization often leads to AttributeErrors when attempting to access instance dependent attributes through self due to the nature of multiprocessings use of pickle. Instead working with functions defined on the module level allows for seamless use of the multiprocessing as it is safe for serialization. As an example, take a look at  the functions defined in the RandomForest source code.</p>"},{"location":"user_guide/scikit_learn/","title":"Using scikit-learn functionality","text":"<p>To simplify integration of adaXT into existing ML workflows based on scikit-learn, adaXT's DecisionTree and RandomForest classes are both designed to be compatible with with some of scikit-learn tools.</p> <p>For example, functions such as GridSearchCV and Pipeline can be used with adaXT.</p>"},{"location":"user_guide/scikit_learn/#using-gridsearchcv","title":"Using GridSearchCV","text":"<p>Here we introduce the difference when using scikit-learn's own DecisionTreeClassifier and adaXT's DecisionTree with the GridSearchCV. First, there is the initial setup:</p> <p><pre><code>from adaXT.decision_tree import DecisionTree\nfrom adaXT.criteria import GiniIndex, Entropy\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.tree import DecisionTreeClassifier\nimport numpy as np\nimport time\n\nn = 20000\nm = 5\n\nX = np.random.uniform(0, 100, (n, m))\nY = np.random.randint(1, 3, n)\n\nparam_grid = {\n    \"max_depth\": [3, 5, 10, 20, 100],\n    \"min_samples_split\": [2, 5, 10],\n}\n\nparam_grid_ada = param_grid | {\"criteria\": [GiniIndex, Entropy]}\nparam_grid_sk = param_grid | {\"criterion\": [\"gini\", \"entropy\"]}\n</code></pre> Here, we import the necessary components and setup the parameter grids of the two decision trees. One small difference to be aware of is that the parameter names and format are different in some cases, e.g., in sklearn it is called criterion and takes a string as input, while in adaXT it is called criteria and takes a criteria class such as GiniIndex, Entropy or perhaps your own implementation. Next, we define and fit the GridSearchCV instance.</p> <p><pre><code>grid_search_ada = GridSearchCV(\n    estimator=DecisionTree(tree_type=\"Classification\"),\n    param_grid=param_grid_ada,\n    cv=5,\n    scoring=\"accuracy\",\n)\n\ngrid_search_sk = GridSearchCV(\n    estimator=DecisionTreeClassifier(),\n    param_grid=param_grid_sk,\n    cv=5,\n    scoring=\"accuracy\",\n)\n\ngrid_search_ada.fit(X, Y)\ngrid_search_sk.fit(X, Y)\n\nprint(\"Best Hyperparameters ada: \", grid_search_ada.best_params_)\nprint(\"Best Hyperparameters sklearn: \", grid_search_sk.best_params_)\nprint(\"Best accuracy ada: \", grid_search_ada.best_score_)\nprint(\"Best accuracy sklearn: \", grid_search_sk.best_score_)\n</code></pre> And that is it. The workflow resembles what you are used to with only a few minor tweaks.</p>"},{"location":"user_guide/scikit_learn/#using-pipeline","title":"Using Pipeline","text":"<p>AdaXT makes it easy to use any preprocessing tools from sklearn because adaXT is compatible with sklearn's Pipeline. An example that combines a scaling step with a decision tree is provided below. Note that while combining a scaling step with a decision tree is generally not needed as decision trees are scale invariant, it can become useful if one additionally  adds a dimensonality reduction step after the scaling, for example. <pre><code>from adaXT.decision_tree import DecisionTree\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n\nX, y = make_classification(random_state=0)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\npipe = Pipeline(\n    [(\"scaler\", StandardScaler()), (\"tree\", DecisionTree(\"Classification\"))]\n)\n\nprint(pipe.fit(X_train, y_train).score(X_test, y_test))\nprint(pipe.set_params(tree__max_depth=5).fit(X_train, y_train).score(X_test, y_test))\n</code></pre></p> <p>Again, there are only minor changes between how the DecisionTree and the DecisionTreeClassifier would be used. The only difference is, that we have to specify, that the DecisionTree is for classification. Instead, one could also pass in a custom criteria, leaf_builder, and predictor and the DecisionTree can still be used as part of a Pipeline.</p>"},{"location":"user_guide/tree_based_weights/","title":"Tree-based weights","text":"<p>Decision trees divide the predictor space into regions, specified by the leaf nodes, such that samples in the same leaf node are close in terms of the criteria used during fitting. As first proposed by Lin and Jeon, 2012 this perspective can be used to view decision trees (and in extension random forests) as adaptive nearest neighbor estimators. To make this more concrete, consider a regression setting with training data \\((X_1, Y_1),\\ldots,(X_n, Y_n)\\in\\mathcal{X}\\times\\mathbb{R}\\), where \\(X_i\\) is a multivariate predictor in \\(\\mathcal{X}\\subseteq\\mathbb{R}^d\\) and \\(Y_i\\) is a real-valued response variable. For a fitted decision tree, we define the leaf function \\(\\mathcal{L}:\\mathcal{X}\\times\\mathcal{X}\\rightarrow{0,1}\\) which determines for all \\(x,x'\\in\\mathcal{X}\\) whether they lie in the same leaf, i.e., \\(\\mathcal{L}(x, x')=1\\) if \\(x\\) and \\(x'\\) are in the same leaf and \\(\\mathcal{L}(x,x')=0\\) if \\(x\\) and \\(x'\\) are in different leafs. Furthermore, define for all \\(i\\in\\{1,\\ldots,n\\}\\) a weight function \\(w_i:\\mathcal{X}\\rightarrow[0,1]\\) for all \\(x\\in\\mathcal{X}\\) by</p> \\[ w_i^{\\operatorname{DT}}(x):=\\frac{\\mathcal{L}(X_{i}, x)}{\\sum_{\\ell=1}^n\\mathcal{L}(X_{\\ell}, x)}. \\] <p>By construction it holds for all \\(x\\in\\mathcal{X}\\) that \\(\\sum_{i=1}^n w_i^{\\operatorname{DT}}(x)=1\\). So intuitively the weights capture how close a new observation \\(x\\) is to each of the training samples. Importantly, the predicted value of a regression tree at the \\(x\\) is simply given by</p> \\[ \\sum_{i=1}^n w_i^{\\operatorname{DT}}(x)Y_i, \\] <p>which corresponds to a weighted nearest neighbor estimator. Unlike other classical weighted nearest neighbor estimators such as kernel smoothers or local polynomial estimators, the weights in this case explicitly depend on the response values making them adaptive.</p> <p>As random forests are just averages over a collection of trees, the discussion above naturally extends to them as well. For a fitted random forest, denote by \\(\\mathcal{L}_1,\\ldots,\\mathcal{L}_M\\) denote the leaf functions for each of the \\(M\\) trees in the forest. Then, for all \\(i\\in\\{1,\\ldots,n\\}\\) and all \\(x\\in\\mathcal{X}\\) define the weights by</p> \\[ w_i^{\\operatorname{RF}}(x):=\\frac{1}{M}\\sum_{m=1}^M\\frac{\\mathcal{L}_m(X_{i}, x)}{\\sum_{\\ell=1}^n\\mathcal{L}_m(X_{\\ell}, x)}. \\] <p>In general \\(w_i^{\\operatorname{RF}}(x)\\) and \\(Y_i\\) are dependent since the sample \\(i\\) appears also in the definition of the weight \\(w_i^{\\operatorname{RF}}(x)\\). This can be avoided using honest splitting, which can be seen as separating the estimation of the weights from the averaging of the responses.</p> <p>Interpreting decision trees and random forests as adaptive nearest neighbor estimators opens the door to applying them in a more diverse set of applications. For example, quantile random forests were first introduced based on this connection (Meinshausen, 2006).</p>"},{"location":"user_guide/tree_based_weights/#tree-based-weights-in-adaxt","title":"Tree-based weights in adaXT","text":"<p>Tree-based weights can be computed in adaXT using the <code>predict_weights</code> method in the DecisionTree and RandomForest classes. The code below illustrates the usage.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom adaXT.decision_tree import DecisionTree\nfrom adaXT.random_forest import RandomForest\n\n# Create toy data set\nn = 100\nX = np.random.uniform(-1, 1, n)\nY = 1.0 * (X &gt; 0) + 2.0 * (X &gt; 0.5) + np.random.normal(0, 0.2, n)\n\n# Fit decision tree and random forest for regression\ntree = DecisionTree(\"Regression\", max_depth=2)\nrf = RandomForest(\"Regression\", max_depth=5)\ntree.fit(X, Y)\nrf.fit(X, Y)\n\n# Extract tree-based weights at training points\n# (X==None uses the training X)\nW_tree = tree.predict_weights()\nW_rf = rf.predict_weights()\n\n# Computing tree-based weights at a new test points\nXtest = np.array([-1, -0.25, 0.25, 1])\nWtest_tree = tree.predict_weights(Xtest)\nWtest_rf = rf.predict_weights(Xtest)\n\n# Rows correspond Xtest while columns always correspond to the training samples\nprint(Wtest_tree.shape)\nprint(Wtest_rf.shape)\n# If scale=True the weights are scaled row-wise\nprint(Wtest_rf.sum(axis=1))\nprint(Wtest_tree.sum(axis=1))\n\n# In the case of regression predicted values correspond to weighted averages of the Y\nYhat_tree = Wtest_tree.dot(Y)\nYhat_rf = Wtest_rf.dot(Y)\nprint(np.c_[Yhat_tree, tree.predict(Xtest)])\nprint(np.c_[Yhat_rf, rf.predict(Xtest)])\n</code></pre> <p>Using the notation above, the weights computed in this example satisfy \\(\\texttt{W_tree}[i, j]=w_j^{\\operatorname{DT}}(\\texttt{X}[i])\\), \\(\\texttt{Wtest_tree}[i, j]=w_j^{\\operatorname{DT}}(\\texttt{Xtest}[i])\\), \\(\\texttt{W_rf}[i, j]=w_j^{\\operatorname{RF}}(\\texttt{X}[i])\\) and \\(\\texttt{Wtest_rf}[i, j]=w_j^{\\operatorname{RF}}(\\texttt{Xtest}[i])\\) for both the decision tree and random forest, respectively.</p>"},{"location":"user_guide/tree_based_weights/#tree-based-weight-induced-similarity","title":"Tree-based weight induced similarity","text":"<p>The tree-based weights can also be used to construct an adaptive measure of closeness in the predictor space. Formally, for two observations \\(x, x'\\in\\mathcal{X}\\), we can define the similarity for decision trees</p> \\[ S^{\\operatorname{DT}}(x, x'):=\\mathcal{L}(x, x'), \\] <p>and for random forests</p> \\[ S^{\\operatorname{RF}}(x, x'):=\\frac{1}{M}\\sum_{m=1}^M\\mathcal{L}_m(x, x'). \\] <p>This is implemented in adaXT via the <code>similarity</code> method that exists for both the DecisionTree and RandomForest class. It allows to easily compute kernel (or Gram) matrices for this similarity.</p> <p>We illustrate this by continuing the example from above.</p> <pre><code># Compute similarity of x values in [-1,1] to test points\ngrid1d = np.linspace(-1, 1, 200)\ntest_points = np.array([-1, -0.25, 0.25, 1])\nsimilarity_tree = tree.similarity(grid1d, test_points)\nsimilarity_rf = rf.similarity(grid1d, test_points)\n\n# Create plot\nfig, ax = plt.subplots(1, 4)\nfor i in range(4):\n    ax[i].plot(grid1d, similarity_tree[:, i], color='blue', label='decision tree', alpha=0.5)\n    ax[i].plot(grid1d, similarity_rf[:, i], color='red', label='random forest', alpha=0.5)\n    ax[i].set_title(f'similarity to x={test_points[i]}')\n    ax[i].legend()\nplt.show()\n</code></pre>"},{"location":"user_guide/vis_and_analysis/","title":"Visualizations and analysis tools","text":""},{"location":"user_guide/vis_and_analysis/#visualizing-decisiontrees","title":"Visualizing DecisionTrees","text":"<p>adaXT provides general plotting functionality with matplotlib. An example of plotting a DecisionTree with a maximum depth of 3 is shown below:</p> <pre><code>from adaXT.decision_tree import DecisionTree, plot_tree\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nN = 10000\nM = 5\nX = np.random.uniform(0, 100, (N, M))\nY = np.random.uniform(0, 4, N)\ntree = DecisionTree(\"Regression\", max_depth=3)\ntree.fit(X, Y)\n\n# Get screen width and height\ndpi = plt.rcParams[\"figure.dpi\"]\n\n\nplt.figure(figsize=(19, 12))\nplot_tree(tree)\nplt.show()\n</code></pre> <p>The resulting plot will look similar to the following:</p> <p></p>"},{"location":"user_guide/vis_and_analysis/#analyzing-the-decisiontree","title":"Analyzing the DecisionTree","text":"<p>adaXT DecisionTrees are built up of individual Nodes. Under the hood, adaXT calls these nodes when predicting. This makes it possible for the user to traverse the tree on their own, such that each individual node can be investigated in more detail. Below we provide an example script, which prints all the nodes in a tree.</p> <pre><code>from adaXT.decision_tree import DecisionNode, LeafNode\nimport numpy as np\n\ndef recurse_node_left(cur_node):\n    if isinstance(cur_node, DecisionNode):\n        print(\"DecisionNode\")\n        print(f\"X{cur_node.split_idx} &lt;= {cur_node.threshold}\")\n        recurse_node_left(cur_node.left_child)\n        recurse_node_left(cur_node.right_child)\n    else:\n      # If not a DecisionNode, then it will always be LeafNode\n      assert(isinstance(cur_node, LeafNode)) \n      print(\"LeafNode\")\n      print(f\"Value: {cur_node.value}\")\n\n\nrecurse_node_left(tree.root)\n</code></pre> <p>Applying this recursive function to the decision tree shown in the previous section, would result in the following output:</p> <pre><code>DecisionNode\nX1 &lt;= 22.623711878856856\nDecisionNode\nX1 &lt;= 22.349316943202385\nDecisionNode\nX3 &lt;= 97.66450448532115\nLeafNode\nValue: [1.92268088]\nLeafNode\nValue: [2.42121542]\nDecisionNode\nX2 &lt;= 6.263581890564451\nLeafNode\nValue: [3.17642358]\nLeafNode\nValue: [1.0559612]\nDecisionNode\nX2 &lt;= 50.61685788523325\nDecisionNode\nX1 &lt;= 25.438273441628738\nLeafNode\nValue: [2.3826478]\nLeafNode\nValue: [2.03267368]\nDecisionNode\nX3 &lt;= 15.113109194828294\nLeafNode\nValue: [2.09236662]\nLeafNode\nValue: [1.95213703]\n</code></pre>"}]}